{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the use of TensorGraph to create graph convolutional models with DeepChem. In particular, we will build a graph convolutional network on the Tox21 dataset.\n",
    "\n",
    "Let's start with some basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import deepchem as dc\n",
    "from deepchem.models.tensorgraph.models.graph_models import GraphConvTensorGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use MoleculeNet to load the Tox21 dataset. We need to make sure to process the data in a way that graph convolutional networks can use For that, we make sure to set the featurizer option to 'GraphConv'. The MoleculeNet call will return a training set, an validation set, and a test set for us to use. The call also returns `transformers`, a list of data transformations that were applied to preprocess the dataset. (Most deep networks are quite finicky and require a set of data transformations to ensure that training proceeds stably.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "# Load Tox21 dataset\n",
    "tox21_tasks, tox21_datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = tox21_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a graph convolutional network on this dataset. DeepChem has the class `GraphConvTensorGraph` that wraps a standard graph convolutional architecture underneath the hood for user convenience. Let's instantiate an object of this class and train it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbharath/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Ending global_step 129: Average loss 586.57\n",
      "TIMING: model fitting took 17.082 s\n"
     ]
    }
   ],
   "source": [
    "model = GraphConvTensorGraph(\n",
    "    len(tox21_tasks), batch_size=50, mode='classification')\n",
    "# Set nb_epoch=10 for better results.\n",
    "model.fit(train_dataset, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to evaluate the performance of the model we've trained. For this, we need to define a metric, a measure of model performance. `dc.metrics` holds a collection of metrics already. For this dataset, it is standard to use the ROC-AUC score, the area under the receiver operating characteristic curve (which measures the tradeoff between precision and recall). Luckily, the ROC-AUC score is already available in DeepChem. \n",
    "\n",
    "To measure the performance of the model under this metric, we can use the convenience function `model.evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.82355634419908874, 0.84317769495469375, 0.83831967490229498, 0.78265073415417696, 0.67514582634690545, 0.78648566994877722, 0.71947133017380849, 0.68636368747841314, 0.77653631284916202, 0.66689869847282113, 0.77872709187364375, 0.74727748235240998]\n",
      "Training ROC-AUC Score: 0.760384\n",
      "computed_metrics: [0.76064908722109537, 0.82394038192827201, 0.83087572150072142, 0.7314288959563835, 0.60984975330966895, 0.60837196235426316, 0.59852764937510705, 0.65035114358925683, 0.71262721074992585, 0.58264411027568919, 0.71303258145363402, 0.66946153846153844]\n",
      "Validation ROC-AUC Score: 0.690980\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = model.evaluate(train_dataset, [metric], transformers)\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores[\"mean-roc_auc_score\"])\n",
    "valid_scores = model.evaluate(valid_dataset, [metric], transformers)\n",
    "print(\"Validation ROC-AUC Score: %f\" % valid_scores[\"mean-roc_auc_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on under the hood? Could we build `GraphConvTensorGraph` ourselves? Of course! The first step is to create a `TensorGraph` object. This object will hold the \"computational graph\" that defines the computation that a graph convolutional network will perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.tensor_graph import TensorGraph\n",
    "\n",
    "tg = TensorGraph(use_queue=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the inputs to our model. Conceptually, graph convolutions just requires a the structure of the molecule in question and a vector of features for every atom that describes the local chemical environment. However in practice, due to TensorFlow's limitations as a general programming environment, we have to have some auxiliary information as well preprocessed.\n",
    "\n",
    "`atom_features` holds a feature vector of length 75 for each atom. The other feature inputs are required to support minibatching in TensorFlow. `degree_slice` is an indexing convenience that makes it easy to locate atoms from all molecules with a given degree. `membership` determines the membership of atoms in molecules (atom `i` belongs to molecule `membership[i]`). `deg_adjs` is a list that contains adjacency lists grouped by atom degree For more details, check out the [code](https://github.com/deepchem/deepchem/blob/master/deepchem/feat/mol_graphs.py).\n",
    "\n",
    "To define feature inputs in `TensorGraph`, we use the `Feature` layer. Conceptually, a `TensorGraph` is a mathematical graph composed of layer objects. `Features` layers have to be the root nodes of the graph since they consitute inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Feature\n",
    "\n",
    "atom_features = Feature(shape=(None, 75))\n",
    "degree_slice = Feature(shape=(None, 2), dtype=tf.int32)\n",
    "membership = Feature(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "deg_adjs = []\n",
    "for i in range(0, 10 + 1):\n",
    "    deg_adj = Feature(shape=(None, i + 1), dtype=tf.int32)\n",
    "    deg_adjs.append(deg_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the body of the graph convolutional network. `TensorGraph` has a number of layers that encode various graph operations. Namely, the `GraphConv`, `GraphPool` and `GraphGather` layers. We will also apply standard neural network layers such as `Dense` and `BatchNorm`.\n",
    "\n",
    "The layers we're adding effect a \"feature transformation\" that will create one vector for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, GraphConv, BatchNorm\n",
    "from deepchem.models.tensorgraph.layers import GraphPool, GraphGather\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "gc1 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[atom_features, degree_slice, membership] + deg_adjs)\n",
    "batch_norm1 = BatchNorm(in_layers=[gc1])\n",
    "gp1 = GraphPool(in_layers=[batch_norm1, degree_slice, membership] + deg_adjs)\n",
    "gc2 = GraphConv(\n",
    "    64,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    in_layers=[gp1, degree_slice, membership] + deg_adjs)\n",
    "batch_norm2 = BatchNorm(in_layers=[gc2])\n",
    "gp2 = GraphPool(in_layers=[batch_norm2, degree_slice, membership] + deg_adjs)\n",
    "dense = Dense(out_channels=128, activation_fn=tf.nn.relu, in_layers=[gp2])\n",
    "batch_norm3 = BatchNorm(in_layers=[dense])\n",
    "readout = GraphGather(\n",
    "    batch_size=batch_size,\n",
    "    activation_fn=tf.nn.tanh,\n",
    "    in_layers=[batch_norm3, degree_slice, membership] + deg_adjs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make predictions from the `TensorGraph` model. Tox21 is a multitask dataset. That is, there are 12 different datasets grouped together, which share many common molecules, but with different outputs for each. As a result, we have to add a separate output layer for each task. We will use a `for` loop over the `tox21_tasks` list to make this happen. We need to add labels for each\n",
    "\n",
    "We also have to define a loss for the model which tells the network the objective to minimize during training.\n",
    "\n",
    "We have to tell `TensorGraph` which layers are outputs with `TensorGraph.add_output(layer)`. Similarly, we tell the network its loss with `TensorGraph.set_loss(loss)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepchem.models.tensorgraph.layers import Dense, SoftMax, \\\n",
    "    SoftMaxCrossEntropy, WeightedError, Concat\n",
    "from deepchem.models.tensorgraph.layers import Label, Weights\n",
    "\n",
    "costs = []\n",
    "labels = []\n",
    "for task in range(len(tox21_tasks)):\n",
    "    classification = Dense(\n",
    "        out_channels=2, activation_fn=None, in_layers=[readout])\n",
    "\n",
    "    softmax = SoftMax(in_layers=[classification])\n",
    "    tg.add_output(softmax)\n",
    "\n",
    "    label = Label(shape=(None, 2))\n",
    "    labels.append(label)\n",
    "    cost = SoftMaxCrossEntropy(in_layers=[label, classification])\n",
    "    costs.append(cost)\n",
    "all_cost = Concat(in_layers=costs, axis=1)\n",
    "weights = Weights(shape=(None, len(tox21_tasks)))\n",
    "loss = WeightedError(in_layers=[all_cost, weights])\n",
    "tg.set_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully defined our graph convolutional model in `TensorGraph`, we need to train it. We can call `fit()`, but we need to make sure that each minibatch of data populates all four `Feature` objects that we've created. For this, we need to create a Python generator that given a batch of data generates a dictionary whose keys are the `Feature` layers and whose values are Numpy arrays we'd like to use for this step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "def data_generator(dataset, epochs=1, predict=False, pad_batches=True):\n",
    "  for epoch in range(epochs):\n",
    "    if not predict:\n",
    "        print('Starting epoch %i' % epoch)\n",
    "    for ind, (X_b, y_b, w_b, ids_b) in enumerate(\n",
    "        dataset.iterbatches(\n",
    "            batch_size, pad_batches=True, deterministic=True)):\n",
    "      d = {}\n",
    "      for index, label in enumerate(labels):\n",
    "        d[label] = to_one_hot(y_b[:, index])\n",
    "      d[weights] = w_b\n",
    "      multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "      d[atom_features] = multiConvMol.get_atom_features()\n",
    "      d[degree_slice] = multiConvMol.deg_slice\n",
    "      d[membership] = multiConvMol.membership\n",
    "      for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "        d[deg_adjs[i - 1]] = multiConvMol.get_deg_adjacency_lists()[i]\n",
    "      yield d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model using `TensorGraph.fit_generator(generator)` which will use the generator we've defined to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Ending global_step 129: Average loss 586.139\n",
      "TIMING: model fitting took 21.158 s\n"
     ]
    }
   ],
   "source": [
    "# Epochs set to 1 to render tutorials online.\n",
    "# Set epochs=10 for better results.\n",
    "tg.fit_generator(data_generator(train_dataset, epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our graph convolutional method, let's evaluate its performance. We again have to use our defined generator to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model\n",
      "computed_metrics: [0.48489674611520039, 0.49062127172873471, 0.49497985043341852, 0.50670994717906093, 0.47636546814871089, 0.50538527032779901, 0.47652531174729351, 0.49406973662015952, 0.49345587239947747, 0.51160175820338205, 0.48764342697532359, 0.505495556795303]\n",
      "Training ROC-AUC Score: 0.493979\n",
      "computed_metrics: [0.49546025306674396, 0.44814469802825652, 0.51528679653679654, 0.50477055883689226, 0.50335989998437258, 0.5068829891838742, 0.57539376819037835, 0.50612128689380276, 0.45944076672265588, 0.58692564745196329, 0.48453889353012158, 0.48315384615384616]\n",
      "Valid ROC-AUC Score: 0.505790\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(\n",
    "    dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
    "\n",
    "print(\"Evaluating model\")\n",
    "train_scores = tg.evaluate_generator(data_generator(train_dataset, predict=True),\n",
    "                                     [metric], labels=labels, weights=[weights])\n",
    "print(\"Training ROC-AUC Score: %f\" % train_scores[\"mean-roc_auc_score\"])\n",
    "valid_scores = tg.evaluate_generator(data_generator(valid_dataset, predict=True),\n",
    "                                     [metric], labels=labels, weights=[weights])\n",
    "print(\"Valid ROC-AUC Score: %f\" % valid_scores[\"mean-roc_auc_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The model we've constructed behaves nearly identically to `GraphConvTensorGraph`. If you're looking to build your own custom models, you can follow the example we've provided here to do so. We hope to see exciting constructions from your end soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
